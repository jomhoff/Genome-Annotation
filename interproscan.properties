#################################
# Distributed mode (Cluster mode)
#################################
####FROM INTERPROSCAN############


# Project name for this run
user.digest=i5GridRun
grid.jobs.limit=1000

# Time between each squeue command to check the status of jobs on the cluster
# Allow master interproscan to run binaries
master.can.run.binaries=true

# Deal with unknown step states
recover.unknown.step.state=false

# Grid submission commands (SLURM sbatch) for starting remote workers
# Commands the master uses to start new remote workers
grid.master.submit.command=sbatch --partition=QUEUE_NAME --mem=8192 --cpus-per-task=4

# Commands a worker uses to start new remote workers
grid.worker.submit.command=sbatch --partition=QUEUE_NAME --mem=8192 --cpus-per-task=4

# Command to start a new worker (new JVM)
worker.command=java -Xms2028M -Xmx9216M -jar interproscan-5.jar
# This may be identical to the worker.command argument above, however you may choose to select
# a machine with a much larger available memory, for use when a StepExecution fails.
worker.high.memory.command=java -Xms2028M -Xmx9216M -jar interproscan-5.jar

# Set the number of embedded workers to the number of processors that you would like to employ
# on the node machine on which the worker will run.
worker.number.of.embedded.workers=20
worker.maxnumber.of.embedded.workers=20

# Max number of connections to the master
master.maxconsumers=48

# Number of connections to the worker
worker.maxconsumers=32

# Throttled network?
grid.throttle=true

# Max number of jobs a tier 1 worker is allowed on its queue
worker.maxunfinished.jobs=32

# Network tier depth
max.tier.depth=1

# Active MQ JMS broker temporary data directory
jms.broker.temp.directory=activemq-data/localhost/tmp_storage
